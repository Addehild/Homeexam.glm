\documentclass[12pt]{article}
\usepackage{graphicx}

\title{Home Exam}
\author{Andreas Hild}
\date{\today}
\renewcommand*\contentsname{Table of contents}
\begin{document}
\maketitle 
\tableofcontents
\newpage

<<include=FALSE>>=
library(dplyr)
library(tidyverse)
library(ggplot2)
library(devtools)
library(modelr)
library(VIM)
@

<<include=TRUE>>==
# Import data
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

# Importing the data
data <- read.csv("MOVIE.csv")
data <- as.tibble(data)

# data <- data %>% filter(original_language %in% c("fr", "de", "it", "no", "uk", "sv", "fi","ne", "es"))

data <- data %>% filter(original_language == "en")


data <- data %>%
  mutate( publ.year = substrRight(as.character(data$release_date), 4))

data <- data %>% filter(publ.year > 2010)


@
\maketitle 
\section{Introduction}
The box office of all movies in the United States for 2018 was worth 11.9 billion USD and is likely to increase over the next year. Therefore it essential for production companies to maximize their chances of getting as many people as possible to go and see a movie in a cinema. This paper aims to investigate how well Poisson and Negative Binomial models fit a dataset of English movies that were released between 2010 to 2017 in order to model the number of paying audiences in a cinema with the help of  Poisson and Negative Binomial Regression models. This in order to see what aspect that influence the popularity of the movie.

https://www.hollywoodreporter.com/news/2018-box-office-revenue-soars-record-119m-us-hits-42b-globally-1172215

\section{Data}
\subsection{Description}
The original dataset used to construct the models for this report consists of data from 45 457 movies and 26 variables of each observation. The publication year of each movie range from 1900 to 2018. However, I have decided to include data from 2010 to 2017 in the study since I cannot verify that older data has been adjusted for inflation which could lead to misleading results. 
The variables which are included in the study are :

belongs to collection whether the movie belongs to a collection or not. 1 indicate Yes and
0 No.
budget the budget of the movie in US dollars.
original language The original first language of the movie.
The popularity variable is the total number of audiences in the cinema for a given movie and will hereafter be refered to as popularity throughout this report. 

\subsection{Exploratory analysis}
The empirical distribution of the popularity variable is drawn in Figure 1. Which show similarities to a small distribution with a small lambda. 



Computing the sample mean and variance for the popularity variable we see that an important property of the Poisson distribution is violated that is equal mean and variance. I will there proceed to use the negative binomial regression and quasi-likelihod estimation in order to account for the violation of this assumption. 
<<>>==
ggplot(data = data, mapping = aes(x = as.numeric(popularity))) +
  geom_density(color= "red") + 
  theme_minimal() + 
  labs(x = "Popularity" , y = "Density")
@

The variable popularity is considered to be count data since each observation measures the number of people at a certain movie. The distribution of counts is discrete, not continuous, and is limited to non-negative values which holds for the popularity variable. 

\subsection{Missing observations}
When exploring the data it is apperent that there is a lot of missing data for the budget variable. 

<<>>==
VIM::aggr(cbind(data$belongs_to_collection, data$budget, data$popularity, data$runtime, data$vote_average, data$vote_count), col=c('green','red'), numbers=TRUE, sortVars=TRUE, labels=c("Collection", "Budget", "Popularity", "Runtime", "Vote Average", "Vote Count"), cex.axis=.7, gap=3, 
                       ylab=c("Proportion of missing","Missing pattern with proportion")) 
@

Looking at the results obtained above, we can clearly see that we are suffering from severe problems of missing data in the budget variable while the other variables have no missing data. As a statistician this raises a lot of concerns since we only have a full dataset of 24 percent of the observations. So in this case we are faced with three different options, the first one is to drop budget variable since there is so much missing data. 

If we assume that the data data is missing completely at random (MCAR), which means that we assume that the probability of being missing is the same for all cases which implies that causes of the missing data are unrelated to the data https://stefvanbuuren.name/fimd/sec-MCAR.html then listwise deletion may be used which in this case would leave us with a dataset of roughly 24 percent of the original dataset. If we however, assume that the data is missing at random (MAR) then we can use an imputation technique such as the EM algorithm. 

<<>>== 
  data2010 <- data %>%
    filter(publ.year == 2010)

  data2011 <- data %>%
    filter(publ.year == 2011)
  
  data2012 <- data %>%
    filter(publ.year == 2012)
 
   data2013 <- data %>%
    filter(publ.year == 2013)
  
   data2014 <- data %>%
     filter(publ.year == 2014)
   
   data2015 <- data %>%
     filter(publ.year == 2015)
   
   data2016 <- data %>%
     filter(publ.year == 2016)
   
   data2017 <- data %>%
     filter(publ.year == 2017)
   data2018 <- data %>%
     filter(publ.year == 2018)
   
   VIM::aggr(cbind(data2010$budget, data2011$budget, data2012$budget, data2013$budget,  
                   data2014$budget, data2015$budget, data2016$budget,
                   data2017$budget,data2018$budget), col=c('green','red'), numbers=TRUE, sortVars=FALSE,
             labels = c(2010,2011,2012,2013,2014,2015,2016,2017,2018))
@

As illustrated above we can see that the proportion of missing data per year is fairly even. Using listwise deletion on a dataset where the distribution and percentage of missing data is large is likely to introduce bias in the results. 

https://www.theanalysisfactor.com/missing-data-mechanism/

From a statistics point of view, the best alternative would in my point of view be to drop the budget variable and imputation techniques would be problematic since there are so many different levels of our categorical values. 

Taking all of this into account, I'm determined to use two approaches to handle the problem. I'll construct models without the budget variable and also a model where I have used listwise deletion on the budget variable. The reason for not just dropping the budget variable is since I believe that budget has the strongest causual relationship to popularity where a high budget would lead to a higher popularity. Likewise, imputation is not performed since the complexity of imputing such high propotion of missing values where most of the information is from categorical values with many levels. For example, the variable actor has 19 045 different levels so even though information about for example director is available, since we have data of 19 045 different directors and imputation is therefore discounted as an option for handeling the missing data. 
\section{Method}

In order to fit a model to predict the popularity of a movie, two models will be constructed. The first model is the overdispersed Poisson model and the second being the negative binomial model. Likewise each type of model will be construced with and without the budget model. 
\subsection{Poisson Regression}
The Poisson regression model is a generalized linear model where the random component is specified by the Poisson distribution of the response variable which is count and discrete. That is Y has a Poisson distribution that is yi Poisson (mu i) for i = 1, ... N and where any set of X = (X1, X2, ...XN) are explanatory variables which can be both categorical and continous. 

https://newonlinecourses.science.psu.edu/stat504/node/165/
\subsection{Negative Binomial Regression}
The Negative Binomial regression is a form of the Poisson regression in that the distributionâ€™s parameter is itself considered to be a random variable. Hence, the variation of this parameter can account for a variance of the data that is higher than the mean. (https://www.theanalysisfactor.com/regression-models-for-count-data/)

The coefficients of a negative binomial regression are obtained by maximum likelihood estimation. 

This suggests that a Negative Binomial Regression could be sutible to model our data since the mean and variance of the popularity variable differed a lot. 

\section{Results}

A potential reason for the not so encouraging results could be due to the sampling techniqe in order to obtain our data. It is stated in the intructions that the data is aggregated from different databases and there could have been different sampling techniques for different databases which could lead to bad results.

   <<>>==
Poisson <- glm( as.numeric(popularity) ~ as.factor(belongs_to_collection) + budget + revenue + vote_average + vote_count+ runtime+ as.factor(publ.year), data=data, family=quasipoisson(link = log) )
@
When running the regression we see that the variable budget is not significant and that the model suffers from overdispersion. 

\subsection{Performance metrics}
\subsection{Conclusions}

The results obtained in this report outline the issues with maximum likelihood estimation methods of regression coefficients. That is that the model accu
https://eranraviv.com/quasi-maximum-likelihood-beauty/

\subsection{Alternative methods}
Zero-inflated regression model and a simple OLS. 

\end{document}
